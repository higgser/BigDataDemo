{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "* http://ampcamp.berkeley.edu/3/exercises/data-exploration-using-spark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing Scala and Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%pyspark\n",
    "print \"Hello\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDDs, DataFrames and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicits allow a quick conversions of Collections and RDDs to Datasets and DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[value: int]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Array(1, 2, 3))\n",
    "rdd.toDS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[value: int]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.makeRDD(1 to 5).toDF(\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoders for most common types are automatically provided by importing sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 3)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seq(1, 2, 3).toDS().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([Manuel,36], [Homer,33])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seq((\"Manuel\", 36), (\"Homer\", 33)).toDF(\"name\", \"age\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([36,Manuel], [33,Homer])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = sc.parallelize(\n",
    "  \"\"\"{\"name\":\"Manuel\",\"age\":36}\"\"\" :: \"\"\"{\"name\":\"Homer\",\"age\":33}\"\"\" :: Nil)\n",
    "sqlContext.read.json(data).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoders are also created for case classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([Manuel,36], [Homer,33])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(name: String, age: Long)\n",
    "val df = Seq(Person(\"Manuel\", 36), Person(\"Homer\", 33)).toDF()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames can be converted to a Dataset by providing a class. The mapping is done by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:24: error: overloaded method value createDataFrame with alternatives:\n",
       "  (data: java.util.List[_],beanClass: Class[_])org.apache.spark.sql.DataFrame <and>\n",
       "  (rdd: org.apache.spark.api.java.JavaRDD[_],beanClass: Class[_])org.apache.spark.sql.DataFrame <and>\n",
       "  (rdd: org.apache.spark.rdd.RDD[_],beanClass: Class[_])org.apache.spark.sql.DataFrame <and>\n",
       "  (rows: java.util.List[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame <and>\n",
       "  (rowRDD: org.apache.spark.api.java.JavaRDD[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame <and>\n",
       "  (rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame\n",
       " cannot be applied to (List[Int], Seq[String])\n",
       "              sqlContext.createDataFrame(List(1,2,3), Seq(\"x\"))\n",
       "                         ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.createDataFrame(List(1,2,3), Seq(\"x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Temp Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|tableName|isTemporary|\n",
      "+---------+-----------+\n",
      "|   person|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.tables.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Manuel| 36|\n",
      "| Homer| 33|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from person\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+------+---+\n",
       "|  name|age|\n",
       "+------+---+\n",
       "|Manuel| 36|\n",
       "+------+---+\n",
       "\n"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql select * from person where age > 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import DataFrames from external sources\n",
    "Load JSON, parquet or Hive tables from local Files, HDFS or S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:18: error: value toDF is not a member of Seq[(String, Int)]\n",
       "         val df = Seq((\"Manuel\", 36), (\"Homer\", 33)).toDF(\"name\", \"age\")\n",
       "                                                     ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq((\"Manuel\", 36), (\"Homer\", 33)).toDF(\"name\", \"age\")\n",
    "df.write.mode(\"overwrite\").json(\"people.json\")\n",
    "val df = sqlContext.read.json(\"people.json\")\n",
    "df.as[Person].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val df = sc.makeRDD(Seq(\"a,b\",\"c,d\")).toDF(\"words\")\n",
    "df.show()\n",
    "df.explode(\"words\", \"word\"){words:String => words.split(\",\")}.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate DataFrames with Map, FlatMap, Reduce, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.reduce((a, b) => a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val textFile = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val wordCount = textFile.map(_.toLowerCase).\n",
    "    flatMap(_.split(\"[ ,.*()?#\\\\[\\\\]]\")).\n",
    "    filter(!_.isEmpty).\n",
    "    map(word => (word,1)).\n",
    "    reduceByKey(_ + _).toDF(\"word\", \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     word|count|\n",
      "+---------+-----+\n",
      "|necessary|    1|\n",
      "|community|    1|\n",
      "|  running|    1|\n",
      "+---------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCount.orderBy(\"count\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### ML, Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values >= 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class' threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (default: 1.0E-6)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (default: )\n",
      "\n",
      "Model 1 was fit using parameters: {\n",
      "\tlogreg_e9c2361e2437-elasticNetParam: 0.0,\n",
      "\tlogreg_e9c2361e2437-featuresCol: features,\n",
      "\tlogreg_e9c2361e2437-fitIntercept: true,\n",
      "\tlogreg_e9c2361e2437-labelCol: label,\n",
      "\tlogreg_e9c2361e2437-maxIter: 10,\n",
      "\tlogreg_e9c2361e2437-predictionCol: prediction,\n",
      "\tlogreg_e9c2361e2437-probabilityCol: probability,\n",
      "\tlogreg_e9c2361e2437-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_e9c2361e2437-regParam: 0.01,\n",
      "\tlogreg_e9c2361e2437-standardization: true,\n",
      "\tlogreg_e9c2361e2437-threshold: 0.5,\n",
      "\tlogreg_e9c2361e2437-tol: 1.0E-6,\n",
      "\tlogreg_e9c2361e2437-weightCol: \n",
      "}\n",
      "Model 2 was fit using parameters: {\n",
      "\tlogreg_e9c2361e2437-elasticNetParam: 0.0,\n",
      "\tlogreg_e9c2361e2437-featuresCol: features,\n",
      "\tlogreg_e9c2361e2437-fitIntercept: true,\n",
      "\tlogreg_e9c2361e2437-labelCol: label,\n",
      "\tlogreg_e9c2361e2437-maxIter: 30,\n",
      "\tlogreg_e9c2361e2437-predictionCol: prediction,\n",
      "\tlogreg_e9c2361e2437-probabilityCol: myProbability,\n",
      "\tlogreg_e9c2361e2437-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_e9c2361e2437-regParam: 0.1,\n",
      "\tlogreg_e9c2361e2437-standardization: true,\n",
      "\tlogreg_e9c2361e2437-threshold: 0.55,\n",
      "\tlogreg_e9c2361e2437-tol: 1.0E-6,\n",
      "\tlogreg_e9c2361e2437-weightCol: \n",
      "}\n",
      "([-1.0,1.5,1.3], 1.0) -> prob=[0.05707304171034024,0.9429269582896597], prediction=1.0\n",
      "([3.0,2.0,-0.1], 0.0) -> prob=[0.9238522311704105,0.07614776882958951], prediction=0.0\n",
      "([0.0,2.2,-1.5], 1.0) -> prob=[0.10972776114779462,0.8902722388522054], prediction=1.0\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "// Prepare training data from a list of (label, features) tuples.\n",
    "val training = sqlContext.createDataFrame(Seq(\n",
    "  (1.0, Vectors.dense(0.0, 1.1, 0.1)),\n",
    "  (0.0, Vectors.dense(2.0, 1.0, -1.0)),\n",
    "  (0.0, Vectors.dense(2.0, 1.3, 1.0)),\n",
    "  (1.0, Vectors.dense(0.0, 1.2, -0.5))\n",
    ")).toDF(\"label\", \"features\")\n",
    "\n",
    "// Create a LogisticRegression instance.  This instance is an Estimator.\n",
    "val lr = new LogisticRegression()\n",
    "// Print out the parameters, documentation, and any default values.\n",
    "println(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "// We may set parameters using setter methods.\n",
    "lr.setMaxIter(10).setRegParam(0.01)\n",
    "\n",
    "// Learn a LogisticRegression model.  This uses the parameters stored in lr.\n",
    "val model1 = lr.fit(training)\n",
    "// Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n",
    "// we can view the parameters it used during fit().\n",
    "// This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    "// LogisticRegression instance.\n",
    "println(\"Model 1 was fit using parameters: \" + model1.parent.extractParamMap)\n",
    "\n",
    "// We may alternatively specify parameters using a ParamMap,\n",
    "// which supports several methods for specifying parameters.\n",
    "val paramMap = ParamMap(lr.maxIter -> 20).\n",
    "  put(lr.maxIter, 30). // Specify 1 Param.  This overwrites the original maxIter.\n",
    "  put(lr.regParam -> 0.1, lr.threshold -> 0.55) // Specify multiple Params.\n",
    "\n",
    "// One can also combine ParamMaps.\n",
    "val paramMap2 = ParamMap(lr.probabilityCol -> \"myProbability\") // Change output column name\n",
    "val paramMapCombined = paramMap ++ paramMap2\n",
    "\n",
    "// Now learn a new model using the paramMapCombined parameters.\n",
    "// paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "val model2 = lr.fit(training, paramMapCombined)\n",
    "println(\"Model 2 was fit using parameters: \" + model2.parent.extractParamMap)\n",
    "\n",
    "// Prepare test data.\n",
    "val test = sqlContext.createDataFrame(Seq(\n",
    "  (1.0, Vectors.dense(-1.0, 1.5, 1.3)),\n",
    "  (0.0, Vectors.dense(3.0, 2.0, -0.1)),\n",
    "  (1.0, Vectors.dense(0.0, 2.2, -1.5))\n",
    ")).toDF(\"label\", \"features\")\n",
    "\n",
    "// Make predictions on test data using the Transformer.transform() method.\n",
    "// LogisticRegression.transform will only use the 'features' column.\n",
    "// Note that model2.transform() outputs a 'myProbability' column instead of the usual\n",
    "// 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "model2.transform(test).\n",
    "  select(\"features\", \"label\", \"myProbability\", \"prediction\").\n",
    "  collect().\n",
    "  foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) =>\n",
    "    println(s\"($features, $label) -> prob=$prob, prediction=$prediction\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.6.0",
   "language": "",
   "name": "spark-scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
